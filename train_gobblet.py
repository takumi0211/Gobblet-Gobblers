import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
from collections import deque, namedtuple
import os
import matplotlib.pyplot as plt
from typing import List, Tuple, Optional, Dict, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod
from tqdm import tqdm

# =============================================================================
# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
# =============================================================================
@dataclass
class HyperParams:
    """å­¦ç¿’ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã¾ã¨ã‚ã¦ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    # --- å­¦ç¿’å…¨ä½“ã®è¨­å®š ---
    NUM_EPISODES: int = 30000        # å­¦ç¿’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°
    LOG_INTERVAL: int = 300          # ãƒ­ã‚°å‡ºåŠ›é–“éš”
    
    # --- DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨­å®š ---
    GAMMA: float = 0.99                # å‰²å¼•ç‡
    EPSILON_START: float = 0.9         # åˆæœŸÎµå€¤ï¼ˆæ¢ç´¢ç‡ï¼‰
    EPSILON_END: float = 0.05          # æœ€çµ‚Îµå€¤
    EPSILON_DECAY: int = 10000         # Îµæ¸›è¡°ã‚¹ãƒ†ãƒƒãƒ—æ•°
    LEARNING_RATE = 5e-4        # å­¦ç¿’ç‡
    BATCH_SIZE = 128            # ãƒãƒƒãƒã‚µã‚¤ã‚º
    TAU = 0.005                 # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°ç‡
    MEMORY_SIZE = 10000         # ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º
    
    # --- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¨­å®š ---
    HIDDEN_SIZE = 128           # éš ã‚Œå±¤ã®ã‚µã‚¤ã‚º
    STATE_DIM = 120             # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°
    
    # --- ãã®ä»–ã®è¨­å®š ---
    GRAD_CLIP_VALUE = 100       # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å€¤
    
    # æœ€é©åŒ–: æ–°ã—ã„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    UPDATE_FREQUENCY = 4        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°é »åº¦
    PRIORITY_REPLAY = False     # å„ªå…ˆåº¦ä»˜ãçµŒé¨“å†ç”Ÿï¼ˆå®Ÿè£…æ™‚ç”¨ï¼‰
    DOUBLE_DQN = True          # Double DQNä½¿ç”¨ãƒ•ãƒ©ã‚°

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«ã™ã‚‹
HP = HyperParams()

def print_hyperparameters():
    """ç¾åœ¨ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã‚’è¡¨ç¤º"""
    print("=" * 60)
    print("ç¾åœ¨ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š")
    print("=" * 60)
    print(f"å­¦ç¿’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°:        {HP.NUM_EPISODES:,}")
    print(f"ãƒ­ã‚°å‡ºåŠ›é–“éš”:           {HP.LOG_INTERVAL:,}")
    print(f"å‰²å¼•ç‡ (Î³):             {HP.GAMMA}")
    print(f"åˆæœŸæ¢ç´¢ç‡ (Îµ_start):    {HP.EPSILON_START}")
    print(f"æœ€çµ‚æ¢ç´¢ç‡ (Îµ_end):      {HP.EPSILON_END}")
    print(f"æ¢ç´¢ç‡æ¸›è¡°ã‚¹ãƒ†ãƒƒãƒ—:      {HP.EPSILON_DECAY:,}")
    print(f"å­¦ç¿’ç‡:                 {HP.LEARNING_RATE}")
    print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º:           {HP.BATCH_SIZE}")
    print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ›´æ–°ç‡ (Ï„):    {HP.TAU}")
    print(f"ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º:  {HP.MEMORY_SIZE:,}")
    print(f"éš ã‚Œå±¤ã‚µã‚¤ã‚º:           {HP.HIDDEN_SIZE}")
    print(f"çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ:        {HP.STATE_DIM}")
    print(f"å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å€¤:      {HP.GRAD_CLIP_VALUE}")
    print("=" * 60)

def update_hyperparameters(**kwargs):
    """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•çš„ã«æ›´æ–°ã™ã‚‹é–¢æ•°
    
    ä½¿ç”¨ä¾‹:
    update_hyperparameters(NUM_EPISODES=50000, LEARNING_RATE=1e-3)
    """
    for key, value in kwargs.items():
        if hasattr(HP, key):
            setattr(HP, key, value)
            print(f"æ›´æ–°: {key} = {value}")
        else:
            print(f"è­¦å‘Š: {key} ã¯æœ‰åŠ¹ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã¯ã‚ã‚Šã¾ã›ã‚“")

def preset_quick_training():
    """ã‚¯ã‚¤ãƒƒã‚¯å­¦ç¿’ç”¨ã®ãƒ—ãƒªã‚»ãƒƒãƒˆï¼ˆçŸ­æ™‚é–“ã§ã®å‹•ä½œç¢ºèªç”¨ï¼‰"""
    update_hyperparameters(
        NUM_EPISODES=5000,
        LOG_INTERVAL=200,
        EPSILON_DECAY=1500
    )
    print("ã‚¯ã‚¤ãƒƒã‚¯å­¦ç¿’ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é©ç”¨ã—ã¾ã—ãŸ")

def preset_strong_ai():
    """å¼·ã„AIå­¦ç¿’ç”¨ã®ãƒ—ãƒªã‚»ãƒƒãƒˆï¼ˆæ™‚é–“ã‚’ã‹ã‘ã¦ã—ã£ã‹ã‚Šå­¦ç¿’ï¼‰"""
    update_hyperparameters(
        NUM_EPISODES=100000,
        LOG_INTERVAL=1000,
        EPSILON_DECAY=30000,
        LEARNING_RATE=1e-4
    )
    print("å¼·ã„AIå­¦ç¿’ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é©ç”¨ã—ã¾ã—ãŸ")

def preset_balanced():
    """ãƒãƒ©ãƒ³ã‚¹å‹å­¦ç¿’ç”¨ã®ãƒ—ãƒªã‚»ãƒƒãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼‰"""
    update_hyperparameters(
        NUM_EPISODES=30000,
        LOG_INTERVAL=300,
        EPSILON_DECAY=10000,
        LEARNING_RATE=5e-4
    )
    print("ãƒãƒ©ãƒ³ã‚¹å‹å­¦ç¿’ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é©ç”¨ã—ã¾ã—ãŸ")

# --- 1. ã‚²ãƒ¼ãƒ ãƒ­ã‚¸ãƒƒã‚¯ ---
class Piece:
    """ã‚³ãƒã‚’è¡¨ã™ã‚¯ãƒ©ã‚¹ - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚slotsä½¿ç”¨"""
    __slots__ = ('color', 'size', '_hash')
    
    def __init__(self, color: str, size: int):
        self.color = color.upper()
        self.size = size
        self._hash = hash((self.color, self.size))
    
    def __str__(self) -> str: 
        return f"{self.color}{self.size}"
    
    def __repr__(self) -> str: 
        return f"Piece('{self.color}', {self.size})"
    
    def __hash__(self) -> int:
        return self._hash
    
    def __eq__(self, other) -> bool:
        if not isinstance(other, Piece):
            return False
        return self.color == other.color and self.size == other.size
    
    def __gt__(self, other):
        if not isinstance(other, Piece): 
            return NotImplemented
        return self.size > other.size

class GobbletGobblersGame:
    """ã‚´ãƒ–ãƒ¬ãƒƒãƒˆã‚´ãƒ–ãƒ©ãƒ¼ã‚ºã®ã‚²ãƒ¼ãƒ ç’°å¢ƒ"""
    
    # ã‚¯ãƒ©ã‚¹å®šæ•°
    BOARD_SIZE = 3
    PIECE_SIZES = [1, 1, 2, 2, 3, 3]
    PLAYERS = ['O', 'B']
    
    def __init__(self):
        # å‹åˆ©åˆ¤å®šç”¨ã®ãƒ©ã‚¤ãƒ³å®šç¾©ï¼ˆäº‹å‰è¨ˆç®—ï¼‰
        self._winning_lines = self._generate_winning_lines()
        # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ç”¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._state_cache = np.zeros(HP.STATE_DIM, dtype=np.float32)
        # ğŸš€ æœ€é©åŒ–1: ã‚³ãƒä½ç½®ã®ç›´æ¥è¿½è·¡ï¼ˆO(1)ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
        self._piece_positions = {}  # piece_id -> (row, col) or 'hand'
        # ğŸš€ æœ€é©åŒ–2: æœ‰åŠ¹æ‰‹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._valid_moves_cache = None
        self._cache_valid = False
        self.reset()

    @staticmethod
    def _generate_winning_lines() -> List[List[Tuple[int, int]]]:
        """å‹åˆ©åˆ¤å®šç”¨ã®ãƒ©ã‚¤ãƒ³åº§æ¨™ã‚’äº‹å‰ç”Ÿæˆ"""
        lines = []
        # æ¨ªã®ãƒ©ã‚¤ãƒ³
        for r in range(3):
            lines.append([(r, c) for c in range(3)])
        # ç¸¦ã®ãƒ©ã‚¤ãƒ³
        for c in range(3):
            lines.append([(r, c) for r in range(3)])
        # æ–œã‚ã®ãƒ©ã‚¤ãƒ³
        lines.append([(i, i) for i in range(3)])
        lines.append([(i, 2 - i) for i in range(3)])
        return lines

    def reset(self):
        """ã‚²ãƒ¼ãƒ ã‚’åˆæœŸçŠ¶æ…‹ã«æˆ»ã™"""
        self.board = [[[] for _ in range(self.BOARD_SIZE)] for _ in range(self.BOARD_SIZE)]
        self.off_board_pieces = {
            player: [Piece(player, size) for size in self.PIECE_SIZES]
            for player in self.PLAYERS
        }
        # çŠ¶æ…‹è¡¨ç¾ç”¨ã«å…¨ã‚³ãƒã®ãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆåˆè¨ˆ12å€‹ï¼‰
        self.all_pieces = self.off_board_pieces['O'] + self.off_board_pieces['B']

        # ğŸš€ æœ€é©åŒ–1: ã‚³ãƒä½ç½®ãƒãƒƒãƒ—ã®åˆæœŸåŒ–
        self._piece_positions = {id(piece): 'hand' for piece in self.all_pieces}
        # ğŸš€ æœ€é©åŒ–2: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–
        self._cache_valid = False

        self.current_player = 'O'
        self.winner = None
        # ğŸ”¥ é‡è¦: åˆæœŸçŠ¶æ…‹ã¯æœ€åˆã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼è¦–ç‚¹ã§è¿”ã™
        return self._get_state_for_player(self.current_player)

    def get_top_piece(self, row: int, col: int) -> Optional[Piece]:
        """æŒ‡å®šã•ã‚ŒãŸãƒã‚¹ã®ä¸€ç•ªä¸Šã®ã‚³ãƒã‚’è¿”ã™"""
        return self.board[row][col][-1] if self.board[row][col] else None

    def switch_player(self):
        """ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’äº¤ä»£ã™ã‚‹"""
        self.current_player = 'B' if self.current_player == 'O' else 'O'

    def check_win(self) -> bool:
        """å‹åˆ©æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹"""
        for line_coords in self._winning_lines:
            pieces = [self.get_top_piece(r, c) for r, c in line_coords]
            if all(pieces) and all(p.color == pieces[0].color for p in pieces):
                self.winner = pieces[0].color
                return True
        return False

    def get_valid_moves(self):
        """ç¾åœ¨ã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå¯èƒ½ãªå…¨ã¦ã®æ‰‹ã‚’ãƒªã‚¹ãƒˆã§è¿”ã™"""
        # ğŸš€ æœ€é©åŒ–: æœ‰åŠ¹æ‰‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        if self._cache_valid and self._valid_moves_cache is not None:
            return self._valid_moves_cache
        
        moves = []
        player = self.current_player

        # 1. é…ç½® (Place) - æ‰‹æŒã¡ã®ã‚³ãƒã‹ã‚‰é‡è¤‡ã‚’é™¤å»ã—ã¦åŠ¹ç‡åŒ–
        available_sizes = sorted(set(p.size for p in self.off_board_pieces[player]))
        for size in available_sizes:
            for r in range(self.BOARD_SIZE):
                for c in range(self.BOARD_SIZE):
                    top_piece = self.get_top_piece(r, c)
                    if top_piece is None or size > top_piece.size:
                        moves.append(('P', size, r, c))
        
        # 2. ç§»å‹• (Move)
        for r_from in range(self.BOARD_SIZE):
            for c_from in range(self.BOARD_SIZE):
                moving_piece = self.get_top_piece(r_from, c_from)
                if moving_piece and moving_piece.color == player:
                    for r_to in range(self.BOARD_SIZE):
                        for c_to in range(self.BOARD_SIZE):
                            if r_from == r_to and c_from == c_to: continue
                            target_piece = self.get_top_piece(r_to, c_to)
                            if target_piece is None or moving_piece > target_piece:
                                moves.append(('M', r_from, c_from, r_to, c_to))
        
        # ğŸš€ æœ€é©åŒ–: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self._valid_moves_cache = moves
        self._cache_valid = True
        return moves
    
    def _get_state(self) -> np.ndarray:
        """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”¨ã®çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        # ğŸš€ æœ€é©åŒ–: O(1)ä½ç½®ã‚¢ã‚¯ã‚»ã‚¹ã«ã‚ˆã‚‹é«˜é€ŸçŠ¶æ…‹ç”Ÿæˆ
        self._state_cache.fill(0.0)
        
        for idx, piece in enumerate(self.all_pieces):
            position = self._piece_positions[id(piece)]
            if position == 'hand':
                location_idx = 9
            else:
                r, c = position
                location_idx = r * 3 + c
            
            self._state_cache[idx * 10 + location_idx] = 1.0
        
        return self._state_cache.copy()

    def _get_state_for_player(self, player: str) -> np.ndarray:
        """ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼è¦–ç‚¹ã®çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆï¼ˆè‡ªåˆ†=1, ç›¸æ‰‹=-1ã§åŒºåˆ¥ï¼‰"""
        self._state_cache.fill(0.0)
        
        for idx, piece in enumerate(self.all_pieces):
            position = self._piece_positions[id(piece)]
            if position == 'hand':
                location_idx = 9
            else:
                r, c = position
                location_idx = r * 3 + c
            
            # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼è¦–ç‚¹ã§å€¤ã‚’è¨­å®šï¼ˆè‡ªåˆ†=1, ç›¸æ‰‹=-1ï¼‰
            value = 1.0 if piece.color == player else -1.0
            self._state_cache[idx * 10 + location_idx] = value
        
        return self._state_cache.copy()

    def step(self, move: Tuple) -> Tuple[np.ndarray, float, bool]:
        """è¡Œå‹•ã‚’å®Ÿè¡Œã—ã€(æ¬¡ã®çŠ¶æ…‹, å ±é…¬, å®Œäº†ãƒ•ãƒ©ã‚°)ã‚’è¿”ã™"""
        player = self.current_player
        move_type = move[0]
        
        if move_type == 'P':
            _, size, r, c = move
            piece_to_place = next(p for p in self.off_board_pieces[player] if p.size == size)
            self.off_board_pieces[player].remove(piece_to_place)
            self.board[r][c].append(piece_to_place)
            # ğŸš€ æœ€é©åŒ–: ä½ç½®è¿½è·¡ã‚’æ›´æ–°
            self._piece_positions[id(piece_to_place)] = (r, c)
        elif move_type == 'M':
            _, r_from, c_from, r_to, c_to = move
            moving_piece = self.board[r_from][c_from].pop()
            self.board[r_to][c_to].append(moving_piece)
            # ğŸš€ æœ€é©åŒ–: ä½ç½®è¿½è·¡ã‚’æ›´æ–°
            self._piece_positions[id(moving_piece)] = (r_to, c_to)
        
        # ğŸš€ æœ€é©åŒ–: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–
        self._cache_valid = False
        
        # ğŸ”¥ é‡è¦: æ‰‹ã‚’æ‰“ã£ãŸç›´å¾Œã«å‹åˆ©åˆ¤å®šï¼ˆãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ‡ã‚Šæ›¿ãˆå‰ï¼‰
        done = self.check_win()
        reward = 0.0
        if done:
            if self.winner == player:  # æ‰‹ã‚’æ‰“ã£ãŸãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå‹åˆ©
                reward = 1.0  # å‹åˆ©
            else:
                reward = -1.0  # æ•—åŒ—ï¼ˆã‚ã‚Šå¾—ãªã„ã‚±ãƒ¼ã‚¹ã ãŒå®‰å…¨ã®ãŸã‚ï¼‰
        
        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ‡ã‚Šæ›¿ãˆ
        self.switch_player()
        
        # ç›¸æ‰‹ã«æœ‰åŠ¹æ‰‹ãŒãªã„å ´åˆã®åˆ¤å®š
        if not done and len(self.get_valid_moves()) == 0:
            # ç›¸æ‰‹ã«æ‰‹ãŒãªã„ = æ‰‹ã‚’æ‰“ã£ãŸãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å‹åˆ©
            done = True
            reward = 1.0
            self.winner = player

        # ğŸ”¥ é‡è¦: æ¬¡ã®çŠ¶æ…‹ã¯æ¬¡ã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼è¦–ç‚¹ã§è¿”ã™
        next_state = self._get_state_for_player(self.current_player)
        return next_state, reward, done

    def display(self):
        """ç¾åœ¨ã®ç›¤é¢ã‚’è¡¨ç¤ºã™ã‚‹ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        print("  | 0 | 1 | 2 |")
        print("--+---+---+---+")
        for i, row in enumerate(self.board):
            print(f"{i} |", end="")
            for c, cell in enumerate(row):
                top_piece = self.get_top_piece(i, c)
                piece_str = str(top_piece) if top_piece else ' '
                print(f" {piece_str:<2} |", end="")
            print("\n--+---+---+---+")
        
        print("\n--- æ‰‹æŒã¡ã®ã‚³ãƒ ---")
        for player, pieces in self.off_board_pieces.items():
            pieces_str = ', '.join(sorted([str(p) for p in pieces]))
            print(f"ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ {player}: {pieces_str}")
        print("\n--------------------")

# --- 2. AIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ ---
Experience = namedtuple('Experience', ('state', 'action_idx', 'reward', 'next_state', 'done'))

class ReplayBuffer:
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)

    def push(self, *args):
        self.memory.append(Experience(*args))

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

class DQN(nn.Module):
    """Deep Q-Network with optimized architecture"""
    
    def __init__(self, n_observations, n_actions):
        super(DQN, self).__init__()
        # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ 
        self.backbone = nn.Sequential(
            nn.Linear(n_observations, HP.HIDDEN_SIZE),
            nn.ReLU(inplace=True),
            nn.Linear(HP.HIDDEN_SIZE, HP.HIDDEN_SIZE),
            nn.ReLU(inplace=True)
        )
        self.value_head = nn.Linear(HP.HIDDEN_SIZE, n_actions)
        
        # é‡ã¿åˆæœŸåŒ–
        self._initialize_weights()
    
    def _initialize_weights(self):
        """é‡ã¿ã®åˆæœŸåŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x):
        features = self.backbone(x)
        return self.value_head(features)

class DQNAgent:
    """æœ€é©åŒ–ã•ã‚ŒãŸDQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, state_dim, action_mapper, player_symbol, device=None):
        self.device = device if device is not None else torch.device("cpu")
        self.state_dim = state_dim
        self.action_mapper = action_mapper
        self.action_dim = len(action_mapper)
        self.player_symbol = player_symbol

        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã‚³ãƒ”ãƒ¼ï¼ˆã‚¢ã‚¯ã‚»ã‚¹é«˜é€ŸåŒ–ï¼‰
        self.gamma = HP.GAMMA
        self.epsilon_start = HP.EPSILON_START
        self.epsilon_end = HP.EPSILON_END
        self.epsilon_decay = HP.EPSILON_DECAY
        self.learning_rate = HP.LEARNING_RATE
        self.batch_size = HP.BATCH_SIZE
        self.tau = HP.TAU

        self.policy_net = DQN(state_dim, self.action_dim).to(self.device)
        self.target_net = DQN(state_dim, self.action_dim).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.learning_rate, amsgrad=True)
        self.memory = ReplayBuffer(HP.MEMORY_SIZE)
        self.steps_done = 0
        
        # è¨ˆç®—åŠ¹ç‡åŒ–ã®ãŸã‚ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._mask_cache = torch.full((self.action_dim,), -float('inf'), device=self.device)
        
        # ğŸš€ æœ€é©åŒ–: ãƒ†ãƒ³ã‚½ãƒ«äº‹å‰ç¢ºä¿
        self._state_tensor_cache = torch.zeros(1, state_dim, device=self.device, dtype=torch.float32)
        self._valid_indices_cache = torch.zeros(self.action_dim, device=self.device, dtype=torch.long)
        
        # ğŸš€ æœ€é©åŒ–: æ··åˆç²¾åº¦å­¦ç¿’ï¼ˆGPUä½¿ç”¨æ™‚ï¼‰
        self.use_amp = device.type == 'cuda' and torch.cuda.is_available()
        if self.use_amp:
            self.scaler = torch.cuda.amp.GradScaler()

        # ğŸš€ æœ€é©åŒ–: æ›´æ–°é »åº¦åˆ¶å¾¡
        self.update_counter = 0

    def select_action(self, state: np.ndarray, valid_moves: List) -> int:
        """è¡Œå‹•é¸æŠ - Îµ-greedyæˆ¦ç•¥"""
        sample = random.random()
        eps_threshold = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \
            np.exp(-1. * self.steps_done / self.epsilon_decay)
        self.steps_done += 1
        
        valid_action_indices = [self.action_mapper.get_action_index(m) for m in valid_moves]

        if sample > eps_threshold:
            with torch.no_grad():
                # ğŸš€ æœ€é©åŒ–: ãƒ†ãƒ³ã‚½ãƒ«å†åˆ©ç”¨
                self._state_tensor_cache[0] = torch.from_numpy(state)
                
                # ğŸš€ æœ€é©åŒ–: æ··åˆç²¾åº¦æ¨è«–
                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        q_values = self.policy_net(self._state_tensor_cache)[0]
                else:
                    q_values = self.policy_net(self._state_tensor_cache)[0]
                # ãƒã‚¹ã‚¯ã‚’å†åˆ©ç”¨
                mask = self._mask_cache.clone()
                mask[valid_action_indices] = 0
                q_values += mask
                action_idx = q_values.argmax().item()
        else:
            action_idx = random.choice(valid_action_indices)
        return action_idx

    def optimize_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ– - ãƒãƒƒãƒå‡¦ç†ã®æ”¹å–„"""
        if len(self.memory) < self.batch_size:
            return
        
        try:
            experiences = self.memory.sample(self.batch_size)
            batch = Experience(*zip(*experiences))

            # ãƒãƒƒãƒãƒ†ãƒ³ã‚½ãƒ«ã®åŠ¹ç‡çš„ãªä½œæˆ
            state_batch = torch.tensor(np.vstack(batch.state), dtype=torch.float32, device=self.device)
            action_batch = torch.tensor(batch.action_idx, dtype=torch.int64, device=self.device).unsqueeze(1)
            reward_batch = torch.tensor(batch.reward, dtype=torch.float32, device=self.device).unsqueeze(1)
            
            non_final_mask = torch.tensor([not done for done in batch.done], dtype=torch.bool, device=self.device)
            next_state_values = torch.zeros(self.batch_size, device=self.device)

            # çµ‚äº†ã—ã¦ã„ãªã„çŠ¶æ…‹ã®ã¿ã‚’å‡¦ç†
            if non_final_mask.any():
                non_final_next_states = torch.tensor(
                    np.vstack([batch.next_state[i] for i in range(len(batch.done)) if not batch.done[i]]), 
                    dtype=torch.float32, device=self.device
                )
                with torch.no_grad():
                    # ğŸš€ æœ€é©åŒ–: Double DQN
                    if HP.DOUBLE_DQN:
                        # Policy networkã§è¡Œå‹•é¸æŠã€Target networkã§ä¾¡å€¤è©•ä¾¡
                        next_actions = self.policy_net(non_final_next_states).argmax(1)
                        next_state_values[non_final_mask] = self.target_net(non_final_next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
                    else:
                        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]
            
            expected_state_action_values = (next_state_values.unsqueeze(1) * self.gamma) + reward_batch
            
            # ğŸš€ æœ€é©åŒ–: æ··åˆç²¾åº¦å­¦ç¿’
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    state_action_values = self.policy_net(state_batch).gather(1, action_batch)
                    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)
                
                self.optimizer.zero_grad()
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), HP.GRAD_CLIP_VALUE)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                state_action_values = self.policy_net(state_batch).gather(1, action_batch)
                loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)
                
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), HP.GRAD_CLIP_VALUE)
                self.optimizer.step()
        except Exception as e:
            print(f"Warning: å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return

    def update_target_net(self):
        target_net_state_dict = self.target_net.state_dict()
        policy_net_state_dict = self.policy_net.state_dict()
        for key in policy_net_state_dict:
            target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + target_net_state_dict[key]*(1-self.tau)
        self.target_net.load_state_dict(target_net_state_dict)

# --- 3. è¡Œå‹•ãƒãƒƒãƒ”ãƒ³ã‚° ---
class ActionMapper:
    def __init__(self):
        self.move_to_idx = {}
        self.idx_to_move = []
        
        # Place
        for size in [1, 2, 3]:
            for r in range(3):
                for c in range(3):
                    move = ('P', size, r, c)
                    if move not in self.move_to_idx:
                        self.move_to_idx[move] = len(self.idx_to_move)
                        self.idx_to_move.append(move)
        # Move
        for r_from in range(3):
            for c_from in range(3):
                for r_to in range(3):
                    for c_to in range(3):
                        if r_from == r_to and c_from == c_to: continue
                        move = ('M', r_from, c_from, r_to, c_to)
                        if move not in self.move_to_idx:
                            self.move_to_idx[move] = len(self.idx_to_move)
                            self.idx_to_move.append(move)

    def get_action_index(self, move): return self.move_to_idx[move]
    def get_move(self, index): return self.idx_to_move[index]
    def __len__(self): return len(self.idx_to_move)

# --- 4. å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã¨ã‚°ãƒ©ãƒ•æç”» ---
def plot_win_rate(win_history, interval):
    """å­¦ç¿’ã®é€²æ—ï¼ˆå‹ç‡ï¼‰ã‚’ã‚°ãƒ©ãƒ•åŒ–ã—ã¦ä¿å­˜ã™ã‚‹"""
    if not win_history: return
    
    episodes = [(i + 1) * interval for i in range(len(win_history))]
    wins_O = [h['O'] for h in win_history]
    wins_B = [h['B'] for h in win_history]
    total_games = [h['O'] + h['B'] for h in win_history]
    
    win_rate_O = [w / t if t > 0 else 0 for w, t in zip(wins_O, total_games)]
    win_rate_B = [w / t if t > 0 else 0 for w, t in zip(wins_B, total_games)]
    
    plt.figure(figsize=(12, 7))
    plt.plot(episodes, win_rate_O, marker='o', linestyle='-', label="Agent 'O' Win Rate (First Player)")
    plt.plot(episodes, win_rate_B, marker='x', linestyle='--', label="Agent 'B' Win Rate (Second Player)")
    
    plt.title('Win Rate History during Self-Play')
    plt.xlabel('Episode')
    plt.ylabel(f'Win Rate over last {interval} episodes')
    plt.grid(True)
    plt.legend()
    plt.ylim(0, 1.0)
    plt.xlim(0, episodes[-1] + interval)
    
    plt.savefig('win_rate_history.png')
    print("\nå­¦ç¿’ã®é€²æ—ã‚°ãƒ©ãƒ•ã‚’ 'win_rate_history.png' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")

def train():
    """æœ€é©åŒ–ã•ã‚ŒãŸå­¦ç¿’ãƒ«ãƒ¼ãƒ—"""
    num_episodes = HP.NUM_EPISODES
    log_interval = HP.LOG_INTERVAL
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
    print_hyperparameters()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    print(f"PyTorch ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.__version__}")
    if torch.cuda.is_available():
        print(f"CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.version.cuda}")
    
    env = GobbletGobblersGame()
    action_mapper = ActionMapper()
    
    agents = {
        'O': DQNAgent(state_dim=HP.STATE_DIM, action_mapper=action_mapper, player_symbol='O', device=device),
        'B': DQNAgent(state_dim=HP.STATE_DIM, action_mapper=action_mapper, player_symbol='B', device=device)
    }
    
    win_history = []
    wins = {'O': 0, 'B': 0}
    episode_rewards = {'O': [], 'B': []}

    for i_episode in tqdm(range(num_episodes), desc="Training episodes", ncols=100):
        state = env.reset()
        done = False
        
        episode_reward = {'O': 0, 'B': 0}

        while not done:
            player = env.current_player
            agent = agents[player]
            
            valid_moves = env.get_valid_moves()
            if not valid_moves:
                done = True
                opponent = 'B' if player == 'O' else 'O'
                wins[opponent] += 1
                continue

            # ğŸ”¥ é‡è¦: ç¾åœ¨ã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼è¦–ç‚¹ã®çŠ¶æ…‹ã‚’å–å¾—
            current_state = env._get_state_for_player(player)
            
            action_idx = agent.select_action(state, valid_moves)
            move = action_mapper.get_move(action_idx)
            
            next_state, reward, done = env.step(move)
            
            # ğŸ”¥ é‡è¦: ç¾åœ¨ã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®çµŒé¨“ã¨ã—ã¦æ­£ã—ãä¿å­˜
            agent.memory.push(current_state, action_idx, reward, next_state, done)
            
            # ğŸ”¥ é‡è¦: ç›¸æ‰‹ãŒæ•—åŒ—ã—ãŸå ´åˆã€ç›¸æ‰‹ã«ã‚‚è² ã®å ±é…¬ã‚’ä¸ãˆã‚‹
            if done and reward == 1.0:
                # å‹åˆ©ã—ãŸãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ç›¸æ‰‹
                opponent = 'B' if player == 'O' else 'O'
                opponent_agent = agents[opponent]
                # ç›¸æ‰‹ã®æœ€å¾Œã®çµŒé¨“ã«è² ã®å ±é…¬ã‚’è¿½åŠ ï¼ˆã‚‚ã—çµŒé¨“ãŒã‚ã‚Œã°ï¼‰
                if len(opponent_agent.memory) > 0:
                    # æœ€å¾Œã®çµŒé¨“ã‚’å–å¾—ã—ã¦è² ã®å ±é…¬ã§æ›´æ–°
                    last_exp = opponent_agent.memory.memory[-1]
                    # æ–°ã—ã„çµŒé¨“ã¨ã—ã¦è² ã®å ±é…¬ã‚’è¿½åŠ 
                    # ç›¸æ‰‹è¦–ç‚¹ã®æœ€çµ‚çŠ¶æ…‹ã‚’å–å¾—
                    final_state_for_opponent = env._get_state_for_player(opponent)
                    opponent_agent.memory.push(last_exp.state, last_exp.action_idx, -1.0, final_state_for_opponent, True)
            
            episode_reward[player] += reward
            
            # ğŸ”¥ ä¿®æ­£: å‹åˆ©ã‚«ã‚¦ãƒ³ãƒˆã®æ­£ç¢ºãªè¨˜éŒ²
            if done and reward == 1.0:
                wins[player] += 1

            # ğŸ”¥ é‡è¦: æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã®ãŸã‚ã«çŠ¶æ…‹ã‚’æ›´æ–°ï¼ˆæ¬¡ã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼è¦–ç‚¹ï¼‰
            if not done:
                state = next_state
            else:
                state = current_state  # ã‚²ãƒ¼ãƒ çµ‚äº†æ™‚ã¯ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿æŒ

            # ğŸš€ æœ€é©åŒ–: æ›´æ–°é »åº¦åˆ¶å¾¡
            agent.update_counter += 1
            if agent.update_counter % HP.UPDATE_FREQUENCY == 0:
                agent.optimize_model()

        for player in ['O', 'B']:
            episode_rewards[player].append(episode_reward[player])
        
        for p_symbol in ['O', 'B']:
            agents[p_symbol].update_target_net()

        if (i_episode + 1) % log_interval == 0:
            avg_reward_O = np.mean(episode_rewards['O'][-log_interval:]) if episode_rewards['O'] else 0
            avg_reward_B = np.mean(episode_rewards['B'][-log_interval:]) if episode_rewards['B'] else 0
            print(f"Episode {i_episode+1}/{num_episodes} | Wins O: {wins['O']}, B: {wins['B']} | Avg Reward O: {avg_reward_O:.3f}, B: {avg_reward_B:.3f}")
            win_history.append({'O': wins['O'], 'B': wins['B']})
            wins = {'O': 0, 'B': 0}

    print("\nTraining finished.")
    
    plot_win_rate(win_history, log_interval)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    try:
        torch.save(agents['O'].policy_net.state_dict(), "dqn_gobblet_agent_O.pth")
        torch.save(agents['B'].policy_net.state_dict(), "dqn_gobblet_agent_B.pth")
        print("ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: dqn_gobblet_agent_O.pth, dqn_gobblet_agent_B.pth")
    except Exception as e:
        print(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    return agents, win_history

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚³ãƒ¼ãƒ‰
    print("=== ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šä¾‹ ===")
    print("# å­¦ç¿’æ™‚é–“ã‚’çŸ­ãã—ãŸã„å ´åˆ:")
    print("# preset_quick_training()  # ã¾ãŸã¯ update_hyperparameters(NUM_EPISODES=5000)")
    print("# ")
    print("# ã‚ˆã‚Šå¼·ã„AIã‚’ä½œã‚ŠãŸã„å ´åˆ:")
    print("# preset_strong_ai()  # ã¾ãŸã¯ update_hyperparameters(NUM_EPISODES=100000)")
    print("# ")
    print("# ãƒãƒ©ãƒ³ã‚¹å‹å­¦ç¿’ã®å ´åˆ:")
    print("# preset_balanced()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š")
    print("# ")
    print("# ç¾åœ¨ã®è¨­å®šã‚’å¤‰æ›´ã™ã‚‹å ´åˆã¯ã€train()ã‚’å‘¼ã¶å‰ã«update_hyperparameters()ã‚’ä½¿ç”¨")
    print()
    
    print("=== çŠ¶æ…‹è¡¨ç¾ã®ãƒ†ã‚¹ãƒˆ ===")
    env = GobbletGobblersGame()
    
    # åˆæœŸçŠ¶æ…‹ã‚’ãƒ†ã‚¹ãƒˆ
    initial_state = env.reset()
    print(f"åˆæœŸçŠ¶æ…‹ã®æ¬¡å…ƒ: {initial_state.shape}")
    print(f"åˆæœŸçŠ¶æ…‹ã®åˆè¨ˆ: {initial_state.sum()}")  # 12å€‹ã®ã‚³ãƒãŒæ‰‹æŒã¡ã«ã‚ã‚‹ã®ã§12ã«ãªã‚‹ã¯ãš
    
    # æ‰‹ã‚’æ‰“ã£ã¦ã¿ã‚‹
    valid_moves = env.get_valid_moves()
    print(f"åˆæœŸã®æœ‰åŠ¹æ‰‹æ•°: {len(valid_moves)}")
    
    # 1æ‰‹æ‰“ã¤
    first_move = valid_moves[0]
    print(f"æœ€åˆã®æ‰‹: {first_move}")
    next_state, reward, done = env.step(first_move)
    print(f"1æ‰‹å¾Œã®çŠ¶æ…‹ã®åˆè¨ˆ: {next_state.sum()}")  # ã¾ã 12ã«ãªã‚‹ã¯ãš
    print(f"å ±é…¬: {reward}, çµ‚äº†: {done}")
    
    # ActionMapperã®ãƒ†ã‚¹ãƒˆ
    action_mapper = ActionMapper()
    print(f"ç·è¡Œå‹•æ•°: {len(action_mapper)}")
    
    print("\n=== å­¦ç¿’é–‹å§‹ ===")
    train()